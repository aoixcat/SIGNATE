{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import pyworld\n",
    "import time\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tools import *\n",
    "from model import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/NTT_corevo\"\n",
    "figure_dir = \"../figure/NTT_corevo/Classifier\"\n",
    "model_dir = \"../model/NTT_corevo/Classifier\"\n",
    "model_name = [\n",
    "    \"Classifier_lr3-4_e10000_b4_label0\",\n",
    "    \"Classifier_lr3-4_e10000_b4_label1\",\n",
    "    \"Classifier_lr3-4_e10000_b4_label2\",\n",
    "    \"Classifier_lr3-4_e10000_b4_label3\",\n",
    "    \"Classifier_lr3-4_e10000_b4_label4\",\n",
    "    \"Classifier_lr3-4_e10000_b4_label5\",\n",
    "    \"Classifier_lr3-4_e10000_b4_child\",\n",
    "    \"Classifier_lr3-4_e10000_b4_adult\",\n",
    "    \"Classifier_lr3-4_e10000_b4_elder\",\n",
    "    \"Classifier_lr3-4_e10000_b4_male\",\n",
    "    \"Classifier_lr3-4_e10000_b4_female\",\n",
    "    \n",
    "]\n",
    "model_dir_vae = \"../model/NTT_corevo/VAE\"\n",
    "model_name_vae = \"VAE_lr3-4_e10000_b4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f71c9cf88d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value = 0\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "num_mcep = 36\n",
    "frame_period = 5.0\n",
    "n_frames = 512\n",
    "label_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(batch_size = 1, label = -1):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    if (label == -1):\n",
    "        random_label = True \n",
    "    else:\n",
    "        random_label =  False\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        if random_label :\n",
    "            label = np.random.randint(0, label_num)\n",
    "            \n",
    "        sample_data_dir = os.path.join(data_dir, \"labeled/{:02}\".format(label))\n",
    "        file = np.random.choice(os.listdir(sample_data_dir))\n",
    "        \n",
    "        frames = 0\n",
    "        count = 0\n",
    "        while frames < n_frames:\n",
    "\n",
    "            wav, _ = librosa.load(os.path.join(sample_data_dir, file), sr = sampling_rate, mono = True)\n",
    "            wav = librosa.util.normalize(wav, norm=np.inf, axis=None)\n",
    "            wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
    "            f0, timeaxis, sp, ap, mc = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period, num_mcep = num_mcep)\n",
    "\n",
    "            if (count == 0):\n",
    "                mc_transposed = np.array(mc).T\n",
    "            else:\n",
    "                mc_transposed = np.concatenate([mc_transposed, np.array(mc).T], axis =1)\n",
    "            frames = np.shape(mc_transposed)[1]\n",
    "\n",
    "            mean = np.mean(mc_transposed)\n",
    "            std = np.std(mc_transposed)\n",
    "            mc_norm = (mc_transposed - mean)/std\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        start_ = np.random.randint(frames - n_frames + 1)\n",
    "        end_ = start_ + n_frames\n",
    "        \n",
    "        data_list.append(mc_norm[:,start_:end_])\n",
    "        label_list.append(label)\n",
    "\n",
    "    return torch.Tensor(data_list).view(batch_size, 1, num_mcep, n_frames), torch.Tensor(label_list).view(batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(losses_list):\n",
    "    if not os.path.exists(figure_dir):\n",
    "            os.makedirs(figure_dir)\n",
    "    losses_list = np.array(losses_list)\n",
    "    plt.figure()\n",
    "    for i in range(len(losses_list)):\n",
    "        losses = losses_list[i]\n",
    "        x = np.linspace(0, len(losses), len(losses))\n",
    "        plt.plot(x, losses, label=\"label : {}\".format(i))\n",
    "        plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0)\n",
    "    plt.savefig(figure_dir + \"/\" + \"_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, label):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, model_name[label]))\n",
    "    \n",
    "def model_load(label):\n",
    "    model = Classifier()\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, model_name[label])))\n",
    "    return model\n",
    "    \n",
    "def model_load_VAE():\n",
    "    model = VAE()\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir_vae, model_name_vae)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "learning_rate_ = 1e-4\n",
    "num_epoch = 10000\n",
    "batch_size = 4\n",
    "\n",
    "num_label = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "  (conv1_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1_gated): Conv2d(1, 8, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "  (conv1_gated_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1_sigmoid): Sigmoid()\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_gated): Conv2d(8, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (conv2_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2_sigmoid): Sigmoid()\n",
       "  (conv3): Conv2d(16, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (conv3_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_gated): Conv2d(16, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (conv3_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3_sigmoid): Sigmoid()\n",
       "  (conv4_mu): Conv2d(16, 5, kernel_size=(9, 5), stride=(9, 1), padding=(1, 2))\n",
       "  (conv4_logvar): Conv2d(16, 5, kernel_size=(9, 5), stride=(9, 1), padding=(1, 2))\n",
       "  (upconv1): ConvTranspose2d(5, 16, kernel_size=(9, 5), stride=(9, 1), padding=(0, 2))\n",
       "  (upconv1_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (upconv1_gated): ConvTranspose2d(5, 16, kernel_size=(9, 5), stride=(9, 1), padding=(0, 2))\n",
       "  (upconv1_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (upconv1_sigmoid): Sigmoid()\n",
       "  (upconv2): ConvTranspose2d(16, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (upconv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (upconv2_gated): ConvTranspose2d(16, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (upconv2_gated_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (upconv2_sigmoid): Sigmoid()\n",
       "  (upconv3): ConvTranspose2d(16, 8, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (upconv3_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (upconv3_gated): ConvTranspose2d(16, 8, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3))\n",
       "  (upconv3_gated_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (upconv3_sigmoid): Sigmoid()\n",
       "  (upconv4_mu): ConvTranspose2d(8, 1, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       "  (upconv4_logvar): ConvTranspose2d(8, 1, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vae = model_load_VAE()\n",
    "model_vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Label 0  :  Epoch 1  :  Loss  0.71724  :  LR 0.00099991\n",
      "Label 0  :  Epoch 2  :  Loss  0.65969  :  LR 0.00099982\n",
      "Label 0  :  Epoch 3  :  Loss  0.66108  :  LR 0.00099973\n",
      "Label 0  :  Epoch 4  :  Loss  0.64219  :  LR 0.00099964\n",
      "Label 0  :  Epoch 5  :  Loss  0.68867  :  LR 0.00099955\n",
      "Label 0  :  Epoch 6  :  Loss  0.83124  :  LR 0.00099946\n",
      "Label 0  :  Epoch 7  :  Loss  0.70824  :  LR 0.00099937\n",
      "Label 0  :  Epoch 8  :  Loss  0.67475  :  LR 0.00099928\n",
      "Label 0  :  Epoch 9  :  Loss  0.68984  :  LR 0.00099919\n",
      "Label 0  :  Epoch 10  :  Loss  0.63004  :  LR 0.0009991\n",
      "Label 0  :  Epoch 11  :  Loss  0.71634  :  LR 0.00099901\n",
      "Label 0  :  Epoch 12  :  Loss  0.63477  :  LR 0.00099892\n",
      "Label 0  :  Epoch 13  :  Loss  0.63702  :  LR 0.00099883\n",
      "Label 0  :  Epoch 14  :  Loss  0.65015  :  LR 0.00099874\n",
      "Label 0  :  Epoch 15  :  Loss  0.96689  :  LR 0.00099865\n",
      "Label 0  :  Epoch 16  :  Loss  0.82843  :  LR 0.00099856\n",
      "Label 0  :  Epoch 17  :  Loss  0.62008  :  LR 0.00099847\n",
      "Label 0  :  Epoch 18  :  Loss  0.85386  :  LR 0.00099838\n",
      "Label 0  :  Epoch 19  :  Loss  0.62935  :  LR 0.00099829\n",
      "Label 0  :  Epoch 20  :  Loss  0.8108  :  LR 0.0009982\n",
      "Label 0  :  Epoch 21  :  Loss  0.59462  :  LR 0.00099811\n",
      "Label 0  :  Epoch 22  :  Loss  0.61973  :  LR 0.00099802\n",
      "Label 0  :  Epoch 23  :  Loss  0.58806  :  LR 0.00099793\n",
      "Label 0  :  Epoch 24  :  Loss  0.82588  :  LR 0.00099784\n",
      "Label 0  :  Epoch 25  :  Loss  0.60533  :  LR 0.00099775\n",
      "Label 0  :  Epoch 26  :  Loss  0.59125  :  LR 0.00099766\n",
      "Label 0  :  Epoch 27  :  Loss  0.58662  :  LR 0.00099757\n",
      "Label 0  :  Epoch 28  :  Loss  0.58888  :  LR 0.00099748\n",
      "Label 0  :  Epoch 29  :  Loss  0.63892  :  LR 0.00099739\n",
      "Label 0  :  Epoch 30  :  Loss  0.75298  :  LR 0.0009973\n",
      "Label 0  :  Epoch 31  :  Loss  0.58922  :  LR 0.00099721\n",
      "Label 0  :  Epoch 32  :  Loss  0.68785  :  LR 0.00099712\n",
      "Label 0  :  Epoch 33  :  Loss  0.56359  :  LR 0.00099703\n",
      "Label 0  :  Epoch 34  :  Loss  0.56169  :  LR 0.00099694\n",
      "Label 0  :  Epoch 35  :  Loss  0.66062  :  LR 0.00099685\n",
      "Label 0  :  Epoch 36  :  Loss  0.57327  :  LR 0.00099676\n",
      "Label 0  :  Epoch 37  :  Loss  0.54314  :  LR 0.00099667\n",
      "Label 0  :  Epoch 38  :  Loss  0.65142  :  LR 0.00099658\n",
      "Label 0  :  Epoch 39  :  Loss  0.55017  :  LR 0.00099649\n",
      "Label 0  :  Epoch 40  :  Loss  0.55719  :  LR 0.0009964\n",
      "Label 0  :  Epoch 41  :  Loss  0.54145  :  LR 0.00099631\n",
      "Label 0  :  Epoch 42  :  Loss  0.57553  :  LR 0.00099622\n",
      "Label 0  :  Epoch 43  :  Loss  0.5497  :  LR 0.00099613\n",
      "Label 0  :  Epoch 44  :  Loss  0.56049  :  LR 0.00099604\n",
      "Label 0  :  Epoch 45  :  Loss  0.46589  :  LR 0.00099595\n",
      "Label 0  :  Epoch 46  :  Loss  0.50917  :  LR 0.00099586\n",
      "Label 0  :  Epoch 47  :  Loss  0.541  :  LR 0.00099577\n",
      "Label 0  :  Epoch 48  :  Loss  0.65812  :  LR 0.00099568\n",
      "Label 0  :  Epoch 49  :  Loss  0.55792  :  LR 0.00099559\n",
      "Label 0  :  Epoch 50  :  Loss  0.57919  :  LR 0.0009955\n",
      "Label 0  :  Epoch 51  :  Loss  0.93308  :  LR 0.00099541\n",
      "Label 0  :  Epoch 52  :  Loss  0.7325  :  LR 0.00099532\n",
      "Label 0  :  Epoch 53  :  Loss  0.55235  :  LR 0.00099523\n",
      "Label 0  :  Epoch 54  :  Loss  0.54888  :  LR 0.00099514\n",
      "Label 0  :  Epoch 55  :  Loss  0.50251  :  LR 0.00099505\n",
      "Label 0  :  Epoch 56  :  Loss  0.54752  :  LR 0.00099496\n",
      "Label 0  :  Epoch 57  :  Loss  0.52149  :  LR 0.00099487\n",
      "Label 0  :  Epoch 58  :  Loss  0.62436  :  LR 0.00099478\n",
      "Label 0  :  Epoch 59  :  Loss  0.50956  :  LR 0.00099469\n",
      "Label 0  :  Epoch 60  :  Loss  0.55305  :  LR 0.0009946\n",
      "Label 0  :  Epoch 61  :  Loss  0.55521  :  LR 0.00099451\n",
      "Label 0  :  Epoch 62  :  Loss  0.51148  :  LR 0.00099442\n",
      "Label 0  :  Epoch 63  :  Loss  0.68734  :  LR 0.00099433\n",
      "Label 0  :  Epoch 64  :  Loss  0.60479  :  LR 0.00099424\n",
      "Label 0  :  Epoch 65  :  Loss  0.60437  :  LR 0.00099415\n",
      "Label 0  :  Epoch 66  :  Loss  0.55386  :  LR 0.00099406\n",
      "Label 0  :  Epoch 67  :  Loss  0.51729  :  LR 0.00099397\n",
      "Label 0  :  Epoch 68  :  Loss  0.51611  :  LR 0.00099388\n",
      "Label 0  :  Epoch 69  :  Loss  0.71666  :  LR 0.00099379\n",
      "Label 0  :  Epoch 70  :  Loss  0.83874  :  LR 0.0009937\n",
      "Label 0  :  Epoch 71  :  Loss  0.52357  :  LR 0.00099361\n",
      "Label 0  :  Epoch 72  :  Loss  0.51046  :  LR 0.00099352\n",
      "Label 0  :  Epoch 73  :  Loss  0.50179  :  LR 0.00099343\n",
      "Label 0  :  Epoch 74  :  Loss  0.51445  :  LR 0.00099334\n",
      "Label 0  :  Epoch 75  :  Loss  0.48592  :  LR 0.00099325\n",
      "Label 0  :  Epoch 76  :  Loss  0.48959  :  LR 0.00099316\n",
      "Label 0  :  Epoch 77  :  Loss  0.4956  :  LR 0.00099307\n",
      "Label 0  :  Epoch 78  :  Loss  0.84856  :  LR 0.00099298\n",
      "Label 0  :  Epoch 79  :  Loss  0.47706  :  LR 0.00099289\n",
      "Label 0  :  Epoch 80  :  Loss  0.47792  :  LR 0.0009928\n",
      "Label 0  :  Epoch 81  :  Loss  0.46807  :  LR 0.00099271\n",
      "Label 0  :  Epoch 82  :  Loss  0.47942  :  LR 0.00099262\n",
      "Label 0  :  Epoch 83  :  Loss  0.47239  :  LR 0.00099253\n",
      "Label 0  :  Epoch 84  :  Loss  0.67026  :  LR 0.00099244\n",
      "Label 0  :  Epoch 85  :  Loss  0.49473  :  LR 0.00099235\n",
      "Label 0  :  Epoch 86  :  Loss  0.4601  :  LR 0.00099226\n",
      "Label 0  :  Epoch 87  :  Loss  0.44998  :  LR 0.00099217\n",
      "Label 0  :  Epoch 88  :  Loss  0.631  :  LR 0.00099208\n",
      "Label 0  :  Epoch 89  :  Loss  0.44259  :  LR 0.00099199\n",
      "Label 0  :  Epoch 90  :  Loss  0.45634  :  LR 0.0009919\n",
      "Label 0  :  Epoch 91  :  Loss  0.55884  :  LR 0.00099181\n",
      "Label 0  :  Epoch 92  :  Loss  0.52073  :  LR 0.00099172\n",
      "Label 0  :  Epoch 93  :  Loss  0.52835  :  LR 0.00099163\n",
      "Label 0  :  Epoch 94  :  Loss  0.7046  :  LR 0.00099154\n",
      "Label 0  :  Epoch 95  :  Loss  0.43566  :  LR 0.00099145\n",
      "Label 0  :  Epoch 96  :  Loss  0.60168  :  LR 0.00099136\n",
      "Label 0  :  Epoch 97  :  Loss  0.58499  :  LR 0.00099127\n",
      "Label 0  :  Epoch 98  :  Loss  0.43994  :  LR 0.00099118\n",
      "Label 0  :  Epoch 99  :  Loss  0.79616  :  LR 0.00099109\n",
      "Label 0  :  Epoch 100  :  Loss  0.51044  :  LR 0.000991\n",
      "Label 0  :  Epoch 101  :  Loss  0.94041  :  LR 0.00099091\n",
      "Label 0  :  Epoch 102  :  Loss  0.53855  :  LR 0.00099082\n",
      "Label 0  :  Epoch 103  :  Loss  0.51276  :  LR 0.00099073\n",
      "Label 0  :  Epoch 104  :  Loss  0.56602  :  LR 0.00099064\n",
      "Label 0  :  Epoch 105  :  Loss  0.43922  :  LR 0.00099055\n",
      "Label 0  :  Epoch 106  :  Loss  0.43819  :  LR 0.00099046\n",
      "Label 0  :  Epoch 107  :  Loss  0.42781  :  LR 0.00099037\n",
      "Label 0  :  Epoch 108  :  Loss  0.74365  :  LR 0.00099028\n",
      "Label 0  :  Epoch 109  :  Loss  0.43608  :  LR 0.00099019\n",
      "Label 0  :  Epoch 110  :  Loss  0.42064  :  LR 0.0009901\n",
      "Label 0  :  Epoch 111  :  Loss  0.44102  :  LR 0.00099001\n",
      "Label 0  :  Epoch 112  :  Loss  0.41962  :  LR 0.00098992\n",
      "Label 0  :  Epoch 113  :  Loss  0.61075  :  LR 0.00098983\n",
      "Label 0  :  Epoch 114  :  Loss  0.5177  :  LR 0.00098974\n",
      "Label 0  :  Epoch 115  :  Loss  0.68749  :  LR 0.00098965\n",
      "Label 0  :  Epoch 116  :  Loss  0.41206  :  LR 0.00098956\n",
      "Label 0  :  Epoch 117  :  Loss  0.52232  :  LR 0.00098947\n",
      "Label 0  :  Epoch 118  :  Loss  0.42411  :  LR 0.00098938\n",
      "Label 0  :  Epoch 119  :  Loss  0.41421  :  LR 0.00098929\n",
      "Label 0  :  Epoch 120  :  Loss  0.75919  :  LR 0.0009892\n",
      "Label 0  :  Epoch 121  :  Loss  0.52628  :  LR 0.00098911\n",
      "Label 0  :  Epoch 122  :  Loss  0.91831  :  LR 0.00098902\n",
      "Label 0  :  Epoch 123  :  Loss  0.65398  :  LR 0.00098893\n",
      "Label 0  :  Epoch 124  :  Loss  0.40246  :  LR 0.00098884\n",
      "Label 0  :  Epoch 125  :  Loss  0.395  :  LR 0.00098875\n",
      "Label 0  :  Epoch 126  :  Loss  0.46793  :  LR 0.00098866\n",
      "Label 0  :  Epoch 127  :  Loss  1.0465  :  LR 0.00098857\n",
      "Label 0  :  Epoch 128  :  Loss  0.40641  :  LR 0.00098848\n",
      "Label 0  :  Epoch 129  :  Loss  0.40798  :  LR 0.00098839\n",
      "Label 0  :  Epoch 130  :  Loss  0.40839  :  LR 0.0009883\n",
      "Label 0  :  Epoch 131  :  Loss  0.40866  :  LR 0.00098821\n",
      "Label 0  :  Epoch 132  :  Loss  0.40908  :  LR 0.00098812\n",
      "Label 0  :  Epoch 133  :  Loss  0.90971  :  LR 0.00098803\n",
      "Label 0  :  Epoch 134  :  Loss  0.45874  :  LR 0.00098794\n",
      "Label 0  :  Epoch 135  :  Loss  0.39672  :  LR 0.00098785\n",
      "Label 0  :  Epoch 136  :  Loss  0.4511  :  LR 0.00098776\n",
      "Label 0  :  Epoch 137  :  Loss  0.5374  :  LR 0.00098767\n",
      "Label 0  :  Epoch 138  :  Loss  0.40688  :  LR 0.00098758\n",
      "Label 0  :  Epoch 139  :  Loss  0.4288  :  LR 0.00098749\n",
      "Label 0  :  Epoch 140  :  Loss  0.39617  :  LR 0.0009874\n",
      "Label 0  :  Epoch 141  :  Loss  0.59381  :  LR 0.00098731\n",
      "Label 0  :  Epoch 142  :  Loss  0.39778  :  LR 0.00098722\n",
      "Label 0  :  Epoch 143  :  Loss  0.78957  :  LR 0.00098713\n",
      "Label 0  :  Epoch 144  :  Loss  0.41687  :  LR 0.00098704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0  :  Epoch 145  :  Loss  0.40074  :  LR 0.00098695\n",
      "Label 0  :  Epoch 146  :  Loss  0.3782  :  LR 0.00098686\n",
      "Label 0  :  Epoch 147  :  Loss  0.53597  :  LR 0.00098677\n",
      "Label 0  :  Epoch 148  :  Loss  0.89408  :  LR 0.00098668\n",
      "Label 0  :  Epoch 149  :  Loss  0.39251  :  LR 0.00098659\n",
      "Label 0  :  Epoch 150  :  Loss  0.61427  :  LR 0.0009865\n",
      "Label 0  :  Epoch 151  :  Loss  0.384  :  LR 0.00098641\n",
      "Label 0  :  Epoch 152  :  Loss  0.68266  :  LR 0.00098632\n",
      "Label 0  :  Epoch 153  :  Loss  1.1504  :  LR 0.00098623\n",
      "Label 0  :  Epoch 154  :  Loss  0.94472  :  LR 0.00098614\n",
      "Label 0  :  Epoch 155  :  Loss  0.38117  :  LR 0.00098605\n",
      "Label 0  :  Epoch 156  :  Loss  0.52416  :  LR 0.00098596\n",
      "Label 0  :  Epoch 157  :  Loss  0.49876  :  LR 0.00098587\n",
      "Label 0  :  Epoch 158  :  Loss  0.35558  :  LR 0.00098578\n",
      "Label 0  :  Epoch 159  :  Loss  0.37203  :  LR 0.00098569\n",
      "Label 0  :  Epoch 160  :  Loss  0.94648  :  LR 0.0009856\n",
      "Label 0  :  Epoch 161  :  Loss  0.35661  :  LR 0.00098551\n",
      "Label 0  :  Epoch 162  :  Loss  0.3513  :  LR 0.00098542\n",
      "Label 0  :  Epoch 163  :  Loss  0.57163  :  LR 0.00098533\n",
      "Label 0  :  Epoch 164  :  Loss  0.34834  :  LR 0.00098524\n",
      "Label 0  :  Epoch 165  :  Loss  0.36353  :  LR 0.00098515\n",
      "Label 0  :  Epoch 166  :  Loss  0.82343  :  LR 0.00098506\n",
      "Label 0  :  Epoch 167  :  Loss  0.56576  :  LR 0.00098497\n",
      "Label 0  :  Epoch 168  :  Loss  0.34711  :  LR 0.00098488\n",
      "Label 0  :  Epoch 169  :  Loss  0.50478  :  LR 0.00098479\n",
      "Label 0  :  Epoch 170  :  Loss  0.34376  :  LR 0.0009847\n",
      "Label 0  :  Epoch 171  :  Loss  0.33945  :  LR 0.00098461\n",
      "Label 0  :  Epoch 172  :  Loss  0.34115  :  LR 0.00098452\n",
      "Label 0  :  Epoch 173  :  Loss  0.90149  :  LR 0.00098443\n",
      "Label 0  :  Epoch 174  :  Loss  0.35085  :  LR 0.00098434\n",
      "Label 0  :  Epoch 175  :  Loss  0.78909  :  LR 0.00098425\n",
      "Label 0  :  Epoch 176  :  Loss  0.6598  :  LR 0.00098416\n",
      "Label 0  :  Epoch 177  :  Loss  0.71186  :  LR 0.00098407\n",
      "Label 0  :  Epoch 178  :  Loss  0.34853  :  LR 0.00098398\n",
      "Label 0  :  Epoch 179  :  Loss  0.33831  :  LR 0.00098389\n",
      "Label 0  :  Epoch 180  :  Loss  0.68601  :  LR 0.0009838\n",
      "Label 0  :  Epoch 181  :  Loss  0.47283  :  LR 0.00098371\n",
      "Label 0  :  Epoch 182  :  Loss  0.4575  :  LR 0.00098362\n",
      "Label 0  :  Epoch 183  :  Loss  0.61656  :  LR 0.00098353\n",
      "Label 0  :  Epoch 184  :  Loss  0.33845  :  LR 0.00098344\n",
      "Label 0  :  Epoch 185  :  Loss  0.31999  :  LR 0.00098335\n",
      "Label 0  :  Epoch 186  :  Loss  0.85325  :  LR 0.00098326\n",
      "Label 0  :  Epoch 187  :  Loss  0.65582  :  LR 0.00098317\n",
      "Label 0  :  Epoch 188  :  Loss  0.70847  :  LR 0.00098308\n",
      "Label 0  :  Epoch 189  :  Loss  0.4482  :  LR 0.00098299\n",
      "Label 0  :  Epoch 190  :  Loss  0.32458  :  LR 0.0009829\n",
      "Label 0  :  Epoch 191  :  Loss  0.47502  :  LR 0.00098281\n",
      "Label 0  :  Epoch 192  :  Loss  0.31342  :  LR 0.00098272\n",
      "Label 0  :  Epoch 193  :  Loss  0.32764  :  LR 0.00098263\n",
      "Label 0  :  Epoch 194  :  Loss  0.65832  :  LR 0.00098254\n",
      "Label 0  :  Epoch 195  :  Loss  0.97783  :  LR 0.00098245\n",
      "Label 0  :  Epoch 196  :  Loss  0.31576  :  LR 0.00098236\n",
      "Label 0  :  Epoch 197  :  Loss  0.31127  :  LR 0.00098227\n",
      "Label 0  :  Epoch 198  :  Loss  0.73542  :  LR 0.00098218\n",
      "Label 0  :  Epoch 199  :  Loss  0.30451  :  LR 0.00098209\n",
      "Label 0  :  Epoch 200  :  Loss  0.65023  :  LR 0.000982\n",
      "Label 0  :  Epoch 201  :  Loss  0.55205  :  LR 0.00098191\n",
      "Label 0  :  Epoch 202  :  Loss  0.53574  :  LR 0.00098182\n",
      "Label 0  :  Epoch 203  :  Loss  0.30879  :  LR 0.00098173\n",
      "Label 0  :  Epoch 204  :  Loss  0.59816  :  LR 0.00098164\n",
      "Label 0  :  Epoch 205  :  Loss  0.46638  :  LR 0.00098155\n",
      "Label 0  :  Epoch 206  :  Loss  1.1453  :  LR 0.00098146\n",
      "Label 0  :  Epoch 207  :  Loss  0.59204  :  LR 0.00098137\n",
      "Label 0  :  Epoch 208  :  Loss  0.29369  :  LR 0.00098128\n",
      "Label 0  :  Epoch 209  :  Loss  0.29946  :  LR 0.00098119\n",
      "Label 0  :  Epoch 210  :  Loss  0.29352  :  LR 0.0009811\n",
      "Label 0  :  Epoch 211  :  Loss  0.48946  :  LR 0.00098101\n",
      "Label 0  :  Epoch 212  :  Loss  0.55274  :  LR 0.00098092\n",
      "Label 0  :  Epoch 213  :  Loss  0.70781  :  LR 0.00098083\n",
      "Label 0  :  Epoch 214  :  Loss  0.74431  :  LR 0.00098074\n",
      "Label 0  :  Epoch 215  :  Loss  0.29557  :  LR 0.00098065\n",
      "Label 0  :  Epoch 216  :  Loss  0.3007  :  LR 0.00098056\n",
      "Label 0  :  Epoch 217  :  Loss  0.29282  :  LR 0.00098047\n",
      "Label 0  :  Epoch 218  :  Loss  0.30927  :  LR 0.00098038\n",
      "Label 0  :  Epoch 219  :  Loss  0.44089  :  LR 0.00098029\n",
      "Label 0  :  Epoch 220  :  Loss  0.33454  :  LR 0.0009802\n",
      "Label 0  :  Epoch 221  :  Loss  0.31161  :  LR 0.00098011\n",
      "Label 0  :  Epoch 222  :  Loss  1.2147  :  LR 0.00098002\n",
      "Label 0  :  Epoch 223  :  Loss  0.29115  :  LR 0.00097993\n",
      "Label 0  :  Epoch 224  :  Loss  0.29393  :  LR 0.00097984\n",
      "Label 0  :  Epoch 225  :  Loss  0.59663  :  LR 0.00097975\n",
      "Label 0  :  Epoch 226  :  Loss  0.30044  :  LR 0.00097966\n",
      "Label 0  :  Epoch 227  :  Loss  0.28689  :  LR 0.00097957\n",
      "Label 0  :  Epoch 228  :  Loss  0.28075  :  LR 0.00097948\n",
      "Label 0  :  Epoch 229  :  Loss  0.29373  :  LR 0.00097939\n",
      "Label 0  :  Epoch 230  :  Loss  0.7485  :  LR 0.0009793\n",
      "Label 0  :  Epoch 231  :  Loss  0.2879  :  LR 0.00097921\n",
      "Label 0  :  Epoch 232  :  Loss  0.61305  :  LR 0.00097912\n",
      "Label 0  :  Epoch 233  :  Loss  0.27148  :  LR 0.00097903\n",
      "Label 0  :  Epoch 234  :  Loss  0.70458  :  LR 0.00097894\n",
      "Label 0  :  Epoch 235  :  Loss  0.42649  :  LR 0.00097885\n",
      "Label 0  :  Epoch 236  :  Loss  0.66173  :  LR 0.00097876\n",
      "Label 0  :  Epoch 237  :  Loss  0.27862  :  LR 0.00097867\n",
      "Label 0  :  Epoch 238  :  Loss  0.62367  :  LR 0.00097858\n",
      "Label 0  :  Epoch 239  :  Loss  0.27394  :  LR 0.00097849\n",
      "Label 0  :  Epoch 240  :  Loss  0.57464  :  LR 0.0009784\n",
      "Label 0  :  Epoch 241  :  Loss  0.26159  :  LR 0.00097831\n",
      "Label 0  :  Epoch 242  :  Loss  0.57896  :  LR 0.00097822\n",
      "Label 0  :  Epoch 243  :  Loss  0.77153  :  LR 0.00097813\n",
      "Label 0  :  Epoch 244  :  Loss  0.25934  :  LR 0.00097804\n",
      "Label 0  :  Epoch 245  :  Loss  0.51517  :  LR 0.00097795\n",
      "Label 0  :  Epoch 246  :  Loss  0.27203  :  LR 0.00097786\n",
      "Label 0  :  Epoch 247  :  Loss  0.55817  :  LR 0.00097777\n",
      "Label 0  :  Epoch 248  :  Loss  0.50699  :  LR 0.00097768\n",
      "Label 0  :  Epoch 249  :  Loss  0.95273  :  LR 0.00097759\n",
      "Label 0  :  Epoch 250  :  Loss  0.28704  :  LR 0.0009775\n",
      "Label 0  :  Epoch 251  :  Loss  0.25755  :  LR 0.00097741\n",
      "Label 0  :  Epoch 252  :  Loss  0.27277  :  LR 0.00097732\n",
      "Label 0  :  Epoch 253  :  Loss  0.27876  :  LR 0.00097723\n",
      "Label 0  :  Epoch 254  :  Loss  0.65199  :  LR 0.00097714\n",
      "Label 0  :  Epoch 255  :  Loss  0.50228  :  LR 0.00097705\n",
      "Label 0  :  Epoch 256  :  Loss  0.27195  :  LR 0.00097696\n",
      "Label 0  :  Epoch 257  :  Loss  0.25919  :  LR 0.00097687\n",
      "Label 0  :  Epoch 258  :  Loss  0.27581  :  LR 0.00097678\n",
      "Label 0  :  Epoch 259  :  Loss  0.75357  :  LR 0.00097669\n",
      "Label 0  :  Epoch 260  :  Loss  0.80567  :  LR 0.0009766\n",
      "Label 0  :  Epoch 261  :  Loss  0.25656  :  LR 0.00097651\n",
      "Label 0  :  Epoch 262  :  Loss  0.54787  :  LR 0.00097642\n",
      "Label 0  :  Epoch 263  :  Loss  0.24879  :  LR 0.00097633\n",
      "Label 0  :  Epoch 264  :  Loss  0.2923  :  LR 0.00097624\n",
      "Label 0  :  Epoch 265  :  Loss  0.85481  :  LR 0.00097615\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "losses_list = []\n",
    "\n",
    "for label in range(num_label):\n",
    "\n",
    "    model = Classifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch += 1\n",
    "        lr = learning_rate_ *(1. / num_epoch) * (epoch) + learning_rate * (1. / num_epoch) * (num_epoch - epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        x_, label_ = data_load(batch_size)\n",
    "        z_ = model_vae.predict(x_)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.calc_loss(z_, label, label_)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Label {}  :  Epoch {}  :  Loss  {:.05}  :  LR {:.05}\". format(label, epoch, loss.item(), lr))\n",
    "\n",
    "    losses_list.append(losses)\n",
    "    save_figure(losses_list)\n",
    "    \n",
    "    model_save(model, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the kind of classifiers\n",
    "- label (1~6)\n",
    "- female or male\n",
    "- chilld or adult or elder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_list = []\n",
    "\n",
    "for episode in range(5):\n",
    "    \n",
    "    # 0 : child\n",
    "    # 1 : adult\n",
    "    # 2 : elder\n",
    "    # 3 : male\n",
    "    # 4 : female\n",
    "\n",
    "    model = Classifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch += 1\n",
    "        lr = learning_rate_ *(1. / num_epoch) * (epoch) + learning_rate * (1. / num_epoch) * (num_epoch - epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        x_, label_ = data_load(batch_size)\n",
    "        z_ = model_vae.predict(x_)\n",
    "        for i in range(batch_size):\n",
    "            # 0 : \"FE_CH\"\n",
    "            # 1 : \"FE_AD\"\n",
    "            # 2 : \"FE_EL\"\n",
    "            # 3 : \"MA_CH\"\n",
    "            # 4 : \"MA_AD\"\n",
    "            # 5 : \"MA_EL\"\n",
    "            if episode == 0:\n",
    "                if label_[i] == 0 or label_[i] == 3:\n",
    "                    label_[i] = 0\n",
    "                else:\n",
    "                    label_[i] = 1\n",
    "            elif episode == 1:\n",
    "                if label_[i] == 1 or label_[i] == 4:\n",
    "                    label_[i] = 0\n",
    "                else:\n",
    "                    label_[i] = 1\n",
    "            elif episode == 2:\n",
    "                if label_[i] == 2 or label_[i] == 5:\n",
    "                    label_[i] = 0\n",
    "                else:\n",
    "                    label_[i] = 1\n",
    "            elif episode == 3:\n",
    "                if label_ [i]== 3 or label_[i] == 4 or label_[i] == 5:\n",
    "                    label_[i] = 0\n",
    "                else:\n",
    "                    label_[i] = 1\n",
    "            elif episode == 4:\n",
    "                if label_[i] == 0 or label_[i] == 1 or label_[i] == 2:\n",
    "                    label_[i] = 0\n",
    "                else:\n",
    "                    label_[i] = 1\n",
    "                    \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.calc_loss(z_, 0, label_)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Label {}  :  Epoch {}  :  Loss  {:.05}  :  LR {:.05}\". format(label, epoch, loss.item(), lr))\n",
    "\n",
    "    losses_list.append(losses)\n",
    "    save_figure(losses_list)\n",
    "    \n",
    "    model_save(model, episode+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
